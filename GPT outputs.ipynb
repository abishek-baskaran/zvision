{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c58f0a3e-7b19-4187-9aa2-374271e07a79",
   "metadata": {},
   "source": [
    "# Step-1: Detailed Documentation\n",
    "\n",
    "Below is a comprehensive, step-by-step guide on how we will create our premium-looking **Customer Entry-Exit Detection** application from scratch. We will target a **Raspberry Pi 5 (4GB)** for final deployment, using an **IP camera** to capture real-time video feeds, and generate analytics to help store owners analyze their **sales/walk-in ratio** and related metrics. The entire development cycle is estimated at **1–2 weeks**. \n",
    "\n",
    "---\n",
    "## 1. Project Concept and Goals\n",
    "\n",
    "1. **Real-time Detection**: We need a system that continuously processes the live feed from an IP camera and detects each customer entering or leaving the store.\n",
    "2. **Analytics & Metrics**: Store owners can view aggregated statistics such as total daily in/out counts, hourly trends, and possibly advanced ratios like sales-to-footfall.\n",
    "3. **Edge Deployment**: The final solution runs on a **Raspberry Pi 5 (4GB)**, ensuring the system can operate at the store location without requiring a large, remote server.\n",
    "\n",
    "---\n",
    "## 2. Preliminary Design Outline\n",
    "\n",
    "1. **Core Components**:\n",
    "   - **Camera Connection**: A streaming client or library that retrieves frames from the IP camera.\n",
    "   - **Detection Module**: An object detection model (e.g., based on YOLO or similar) to detect people in frames.\n",
    "   - **Tracking & Counting**: Logic to maintain each detected object’s identity, track movements, and label them as “entering” or “exiting” based on boundary crossing.\n",
    "   - **Database & Analytics**: A place to store each event and generate aggregated metrics (daily counts, peak hours, etc.).\n",
    "   - **Frontend & Display**: Simple or “premium” interface that can be accessed locally or via network, showing real-time counts and historical data with a Cupertino-style or other modern UI design.\n",
    "\n",
    "2. **Data Flow** (High-level):\n",
    "   1. **IP Camera → Frame Grab**  \n",
    "      The Raspberry Pi (or your dev machine) retrieves the video feed.\n",
    "   2. **Object Detection**  \n",
    "      Each video frame is passed into a detection model to locate and classify people.\n",
    "   3. **Boundary/Line Crossing**  \n",
    "      A geometric logic determines if the person crossed an “in” line, an “out” line, or stayed inside a region of interest.\n",
    "   4. **Count & Log**  \n",
    "      Each event is logged (with timestamp, direction in/out, and possibly short clips or snapshots if needed).\n",
    "   5. **Analytics**  \n",
    "      Aggregated data is processed (hourly totals, daily, monthly, etc.) and displayed to the store owner.\n",
    "\n",
    "---\n",
    "## 3. Development Schedule (Approx. 1–2 Weeks)\n",
    "\n",
    "### Days 1–3: Environment Setup & File Structure\n",
    "\n",
    "1. **Prepare Your Dev Environment (Windows System + Anaconda + VS Code)**:\n",
    "   - Create a dedicated project folder. For instance: `edge_detection_app`.\n",
    "   - Make a new environment (to avoid conflicts with other packages). For example:\n",
    "     - Use something like:  \n",
    "       `conda create --name edge_app python=3.9`\n",
    "     - Then activate it:  \n",
    "       `conda activate edge_app`\n",
    "   - Install any critical libraries we expect to use:  \n",
    "       - Example: `pip install fastapi uvicorn opencv-python numpy`  \n",
    "       - If using YOLO-based approaches, we might install `ultralytics` or another detection framework.\n",
    "   - Confirm that your Python, pip, and environment are set up correctly.\n",
    "\n",
    "2. **Plan Directory Layout**:  \n",
    "   - **app/**  \n",
    "     - `main.py` (entry point for our FastAPI or any server code)  \n",
    "     - `camera_client.py` (manages IP camera connection)  \n",
    "     - `detection.py` (YOLO or other detection logic)  \n",
    "     - `counter.py` (tracking and line-crossing logic)  \n",
    "     - `analytics.py` (summaries and stats generation)  \n",
    "     - `database.py` (DB connection and queries)  \n",
    "     - `routes/` (organized FastAPI routes, if we prefer separate files)  \n",
    "   - **models/**  \n",
    "     - Pydantic schemas or data classes.  \n",
    "   - **static/** or **frontend/** (if any UI assets are needed)  \n",
    "   - **tests/** (test suite for each module)\n",
    "\n",
    "3. **Raspberry Pi Preparation** (Planned, not done yet):\n",
    "   - Once the Pi arrives (in about 3–4 days), we’ll replicate the environment on it.\n",
    "   - We can either replicate the exact conda environment or use a simpler approach (e.g., `pip` in a virtualenv) to ensure minimal overhead on the Pi.\n",
    "\n",
    "### Days 4–7: Core Detection & Counting Logic (High-Level Future Tasks)\n",
    "- (Not implementing yet—just a placeholder for the plan.)\n",
    "- Integrate the camera feed, run object detection on each frame, track recognized bounding boxes, and decide in/out statuses.\n",
    "\n",
    "### Days 8–12: UI & Analytics (High-Level Future Tasks)\n",
    "- (Placeholder for planning.)\n",
    "- Build a modern interface or an API returning JSON endpoints that a “premium-style” front-end can consume. Provide analytics, charts, and peak hour metrics.\n",
    "\n",
    "---\n",
    "## 4. Step-by-Step Guide (Expanded)\n",
    "\n",
    "1. **Setting Up the Dev Environment**:\n",
    "   - On Windows, open **VS Code** and your chosen folder (`edge_detection_app`).\n",
    "   - Start a terminal in VS Code, activate your environment, and confirm everything with `python --version`.\n",
    "\n",
    "2. **Establish a Starter FastAPI Project**:\n",
    "   - Initialize a `main.py` with the bare bones of a FastAPI server:\n",
    "     - ```python\n",
    "       from fastapi import FastAPI\n",
    "\n",
    "       app = FastAPI()\n",
    "\n",
    "       @app.get(\"/\")\n",
    "       def read_root():\n",
    "           return {\"message\": \"Hello from Edge Detection App!\"}\n",
    "       ```\n",
    "   - Test run with `uvicorn main:app --reload`. Verify in your browser at `http://127.0.0.1:8000`.\n",
    "\n",
    "3. **Plan for the Database**:\n",
    "   - Depending on your preference, set up either:\n",
    "     - A lightweight **SQLite** database if your local storage needs are minimal.\n",
    "     - A remote or local **PostgreSQL** for advanced features, especially if big data or concurrency is expected.\n",
    "   - For now, plan to create a `database.py` with functions for connecting and performing basic CRUD operations.\n",
    "\n",
    "4. **Outline the Detection Approach**:\n",
    "   - Decide on a detection model. For example, if we use **ultralytics YOLO**, we can do:\n",
    "     - `pip install ultralytics`\n",
    "     - Then in `detection.py`:\n",
    "       ```python\n",
    "       from ultralytics import YOLO\n",
    "\n",
    "       def load_model():\n",
    "           model = YOLO('yolov8n.pt')  # or a custom model\n",
    "           return model\n",
    "       ```\n",
    "   - Outline how you’ll capture frames from the IP camera (e.g., `rtsp://` URL). Keep in mind that we’ll finalize this when we have the Pi + camera.\n",
    "\n",
    "5. **Prototype Tracking & Counting**:\n",
    "   - In `counter.py`, write a class or function that:\n",
    "     - Receives frames, identifies bounding boxes for people, and uses a “tracking ID” approach (like a built-in YOLO tracker or custom).\n",
    "     - Identifies if a bounding box crosses a specific line or polygon boundary for “entry” vs. “exit.”\n",
    "     - Logs events to the database.\n",
    "\n",
    "6. **Validation & Local Testing**:\n",
    "   - Use a test video feed (locally stored or a dummy RTSP stream) to ensure the detection pipeline works well on your Windows machine.\n",
    "\n",
    "7. **Raspberry Pi Deployment** (Days 4+ or once Pi is available):\n",
    "   - Transfer the code to the Pi.\n",
    "   - Install Python dependencies (consider whether conda is practical or if you prefer a simpler approach).\n",
    "   - Adjust the detection logic if any performance constraints arise (might switch to a smaller model or optimize settings).\n",
    "\n",
    "8. **Analytics Implementation**:\n",
    "   - Create a route or method that aggregates data by day/hour and returns the results in JSON (or renders via a UI template).\n",
    "   - Possibly integrate charts (via a web UI) so the store owner can see real-time in/out data, daily peaks, or advanced stats.\n",
    "\n",
    "---\n",
    "## 5. Project Focus for Days 1–3\n",
    "\n",
    "1. **Day 1**:\n",
    "   - Confirm environment setup, build skeleton folder structure.\n",
    "   - Initialize `main.py` with a basic FastAPI app.\n",
    "   - Discuss or decide on the detection framework (YOLO, etc.).\n",
    "\n",
    "2. **Day 2**:\n",
    "   - Implement a minimal `database.py` with a simple SQLite or other DB connection.\n",
    "   - Add placeholders for `detection.py` and `counter.py`.\n",
    "   - Make sure you can run `uvicorn main:app --reload` with no errors.\n",
    "\n",
    "3. **Day 3**:\n",
    "   - Test a simple local video detection flow.\n",
    "   - Finalize how we’ll handle IP camera streams in code (test a placeholder RTSP link).\n",
    "   - Prepare all placeholders so that once the Raspberry Pi arrives, we can quickly integrate and test on the actual device + camera.\n",
    "\n",
    "---\n",
    "## 6. Next Steps\n",
    "\n",
    "This document gives a thorough, high-level blueprint of how we’ll create the **new application** from scratch. Our next steps after finalizing this plan include:\n",
    "\n",
    "- Completing **Day 1–3** tasks (environment, skeleton code, local testing).\n",
    "- Extending the codebase with real detection, tracking, database logging, and eventually analytics once the Pi and camera are available.\n",
    "\n",
    "This concludes **Step-1**: our detailed documentation and plan to move forward. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f660c-a254-4db8-b014-ee8c6a1f4ff8",
   "metadata": {},
   "source": [
    "**question**: What database solution do you prefer for storing event logs—SQLite, PostgreSQL, or something else? \\\n",
    "**answer**: I don't have any preference. My plan is to have the raspberry pi for inference and video storage, while the logs are pushed online such as firebase (if it is feasible).\n",
    "\n",
    "**question**: How do you plan to access the analytics dashboard (web browser on local network, remote access, etc.)? \\\n",
    "**answer**: Analytics dashboard on the mobile app. \n",
    "\n",
    "**question**: Do you have specific security or authentication requirements for the dashboard (e.g., user roles, login system)? \\\n",
    "**answer**: No. The app will be deployed only to the admin who will have access to all features.\n",
    "\n",
    "**question**: Will the analytics include only basic counts and hourly/daily summaries, or do you need more advanced statistics (e.g., dwell times, repeat visits)? \\\n",
    "**answer**: I need more advanced summaries. But, note that, I don't want a complicated system with re-identification.\n",
    "\n",
    "**question**: What resolution and frame rate can your IP camera reliably provide over your network? \\\n",
    "**answer**: It provides 30FPS 1080P resolution. If the edge device cannot handle, please make sure, downscaling and downrating are done.\n",
    "\n",
    "**question**: Are there any constraints on CPU usage, GPU availability, or power consumption for the Raspberry Pi? \\\n",
    "**answer**: Get the specs of raspberry pi 5 4GB device as it will be dedicated only for this task.\n",
    "\n",
    "**question**: Do you want the system to store short video clips of each detected entry/exit event, or just text-based logs? \\\n",
    "**answer**: Yes, I need to store short clips\n",
    "\n",
    "**question**: Which object detection model are you comfortable with—YOLO variants, MobileNet, or do you have a specific preference? \\\n",
    "**answer**: YOLO is fine as I have tested it. Not sure about MobileNet or other models.\n",
    "\n",
    "**question**: Do you expect to run additional processes (like POS software) on the Raspberry Pi, which might compete for resources? \\\n",
    "**answer**: No. Edge device is dedicated to this single application.\n",
    "\n",
    "**question**: Should we provide a fallback or offline mode if the network connection to the IP camera is unstable? \\\n",
    "**answer**: Not sure about it.\n",
    "\n",
    "**question**: How long do you need to retain data (e.g., logs, analytics) before archiving or purging? \\\n",
    "**answer**: Not sure about it.\n",
    "\n",
    "**question**: Do you need any integration with other systems, such as a sales database, to correlate footfall with sales data? \\\n",
    "**answer**: Not right now, but it is on the cards. I would like the code to be modular, so that I can just plug it in when needed.\n",
    "\n",
    "**question**: Will you require any custom UI design elements beyond the Cupertino-themed widgets mentioned (e.g., brand colors, logos)? \\\n",
    "**answer**: Not right now.\n",
    "\n",
    "**question**: Are there any privacy or data-protection regulations (like GDPR) that we need to consider, especially for captured videos or images? \\\n",
    "**answer**: Not sure about that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c0f8bd-d5b7-443a-b712-27011579f1cc",
   "metadata": {},
   "source": [
    "**question**: Since you plan to use Firebase, are you comfortable with potential subscription costs or data limits, and do you already have a Firebase account set up? \\\n",
    "**answer**: Isn't the spark (free) plan sufficient for this? If not, I would easily have the DB in the raspberry pi using other DB.\n",
    "\n",
    "**question**: For storing short video clips locally on the Pi, how much storage capacity is available, and do we need an automated cleanup policy? \\\n",
    "**answer**: Yes. I need a threshold like 1000 clips, followed by first-in-first-out cleanup policy.\n",
    "\n",
    "**question**: Should the mobile app be a native iOS/Android app, or a cross-platform framework, and do you already have a preferred approach? \\\n",
    "**answer**: We'll start with android app.\n",
    "\n",
    "**question**: Do you need real-time notifications (e.g., push alerts on your phone) whenever a person enters or exits? \\\n",
    "**answer**: Not whenever person enters or exits. But, need alerts scenarios like but not limited to when unusual footfall detection. \n",
    "\n",
    "**question**: Would you like to anonymize video clips in any way (e.g., face blurring) or store them as-is? \\\n",
    "**answer**: Store them as it is.\n",
    "\n",
    "**question**: For advanced summaries, which metrics are highest priority (e.g., average visit duration, peak traffic hours, conversion rate if we have sales data later)? \\\n",
    "**answer**: You do a deep brainstorm and give a set of metrics that makes sense.\n",
    "\n",
    "**question**: Could you clarify if you’d like an on-device UI (e.g., touchscreen on the Pi) in addition to the mobile app, or will the Pi only run headless? \\\n",
    "**answer**: No. I need Pi to be headless.\n",
    "\n",
    "**question**: Should the system prompt you to manually update or automatically pull updates from a repository (for feature enhancements or bug fixes)? \\\n",
    "**answer**: Auto update.\n",
    "\n",
    "**question**: What network setup do you have at the store—wired Ethernet, reliable Wi-Fi, or do we need fallback to something like a 4G dongle? \\\n",
    "**answer**: Reliable fiber connection with wifi.\n",
    "\n",
    "**question**: For downscaling video, do you have a target resolution or frame rate in mind to ensure smooth object detection on the Pi? \\\n",
    "**answer**: No, let it be a hyperparameter.\n",
    "\n",
    "**question**: When storing event logs in Firebase, do you also want local caching of logs if internet is unavailable, and then push them when online again? \\\n",
    "**answer**: Yes. I need it. (Even if firebase is not used.)\n",
    "\n",
    "**question**: Do you plan to maintain multiple IP cameras in the future (e.g., multiple store entrances) under one Pi device? \\\n",
    "**answer**: Yes. I was about to ask you if it is possible. If multiple entrance can be processed on one pi device, I would buy 8GB instead of 4GB device."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2501455-158a-4b0e-b040-c33f7ae5c79e",
   "metadata": {},
   "source": [
    "**question**: Would you like to keep logs for each camera separately, or aggregate them in one unified dataset when you have multiple entrances? \\\n",
    "**answer**: Keep logs of each camera separately.\n",
    "\n",
    "**question**: If you opt for 8GB instead of 4GB on the Pi, do you have any additional performance targets—like higher resolution processing, or more AI models running in parallel? \\\n",
    "**answer**: Parallel running is necessary right? To run multiple entrance simultaneously. I don't have any other performance target like higher res processing.\n",
    "\n",
    "**question**: For unusual footfall detection alerts, do you want basic thresholds (e.g., X people in Y minutes) or something more sophisticated (e.g., sudden surges based on historical average)? \\\n",
    "**answer**: Basic is fine.\n",
    "\n",
    "**question**: Regarding the clips stored on the Pi, should we also consider automatically uploading them to the cloud (Firebase or elsewhere) after each event, or only keep them locally? \\\n",
    "**answer**: No, let it just be local. \n",
    "\n",
    "**question**: If the app runs on Android, do you want any offline capabilities there (caching old analytics or logs on the device), or is an online connection required to view analytics? \\\n",
    "**answer**: Offline cache should be there.\n",
    "\n",
    "**question**: Are there any UI preferences for the mobile app beyond Cupertino-style widgets, such as specific color schemes or minimalistic layouts? \\\n",
    "**answer**: We'll discuss during building the application.\n",
    "\n",
    "**question**: For the advanced summaries you mentioned, do you want daily/hourly segmentation, trends over weeks/months, or real-time charts that refresh as events occur? \\\n",
    "**answer**: I am not sure about it now.\n",
    "\n",
    "**question**: What level of error tolerance do you envision for the detection? For instance, do you allow for some false positives/negatives, or do you need near 100% accuracy? \\\n",
    "**answer**: I need near 100% accuracy with negligable FP/FN.\n",
    "\n",
    "**question**: If you decide to add sales data later, do you have a preferred POS or database format we should anticipate integrating with? \\\n",
    "**answer**: Not sure, but, let's stick for csv now as it is simple.\n",
    "\n",
    "**question**: Finally, is there any scenario where the Raspberry Pi will need a display in future (e.g., quick local debugging), or shall we always assume strict headless operation? \\\n",
    "**answer**: No, just headless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38a2be5-46b9-45ea-b043-a8086d268080",
   "metadata": {},
   "source": [
    "**question**: Does the store layout or camera positioning present any challenging angles or lighting conditions (e.g., glare, reflections, partially obstructed entrances) that we should account for? \\\n",
    "**answer**: No.\n",
    "\n",
    "**question**: Would you like to exclude staff from the counting or otherwise differentiate them (if feasible) to get more accurate customer-only data? \\\n",
    "**answer**: Not right now. But as the video capture grows, I will start labeling staff / non-staff and do fine-tuning to train on the store data for the classification.\n",
    "\n",
    "**question**: Do you anticipate high-density crowds (e.g., a rush hour), and should the system handle partial occlusions or group entries? \\\n",
    "**answer**: Max 3-4 people at any point of time walking through the store.\n",
    "\n",
    "**question**: Are there any constraints related to internet bandwidth for sending logs/clips, or is your connection robust enough for all planned operations? \\\n",
    "**answer**: Bandwidth is robust.\n",
    "\n",
    "**question**: Do you want a simple calibration step upon setup (e.g., specify the entrance lines in a visual interface) or should we hard-code the region of interest? \\\n",
    "**answer**: I need the calibration step.\n",
    "\n",
    "**question**: Since you aim for near 100% detection accuracy, are you open to iterative tuning (e.g., fine-tuning the model on store-specific data) if the default YOLO model underperforms? \\\n",
    "**answer**: Yes. (Refer answer of question-2 in this message.)\n",
    "\n",
    "**question**: How do you plan to manage user support or troubleshooting? Should we provide a remote debugging interface or logs for diagnostics? \\\n",
    "**answer**: Not sure.\n",
    "\n",
    "**question**: If multiple cameras stream to the same Pi, do you want each feed processed in parallel on separate threads/processes for real-time performance? \\\n",
    "**answer**: Not sure.\n",
    "\n",
    "**question**: Do you want any kind of tamper detection (e.g., alerts if the camera is covered or disconnected)? \\\n",
    "**answer**: Not right now.\n",
    "\n",
    "**question**: Finally, are there any expansion ideas or future plans (e.g., face recognition for VIP customers, cross-store analytics) that might influence our initial architecture? \\\n",
    "**answer**: Yes maybe, but don't focus on it right now.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fbcda6-41e1-4ce0-bce3-f7e6c9fd7e6c",
   "metadata": {},
   "source": [
    "## Summary of Requirements and Preferences\n",
    "\n",
    "1. **Data Storage & Backend**\n",
    "   - Primary logs can be **pushed online** (Firebase) if feasible; otherwise, use a local database on the Pi.\n",
    "   - **Short video clips** will be stored locally on the Pi, with a **FIFO cleanup** policy after reaching 1000 clips.\n",
    "\n",
    "2. **Mobile App & Analytics**\n",
    "   - Analytics on an **Android** mobile app (Cupertino-style UI discussion later).\n",
    "   - Requires **offline caching** of analytics/logs when no internet.\n",
    "   - No complicated authentication—admin has all access.\n",
    "   - **Advanced summaries** beyond basic counts, but **no re-identification** for staff vs. customers initially.\n",
    "   - Potential for **iterative tuning** with store-specific data to improve accuracy.\n",
    "\n",
    "3. **Hardware & Performance**\n",
    "   - Target device is a **Raspberry Pi 5** (4GB or 8GB). Pi runs **headless** and is dedicated solely to this task.\n",
    "   - **IP camera** provides **1080p at 30FPS**; plan to **downscale**/downrate if needed for performance.\n",
    "   - **Multiple camera feeds** may be handled in parallel if the Pi resources allow.\n",
    "\n",
    "4. **Detection & Features**\n",
    "   - Uses **YOLO** for object detection. Aiming for **near 100%** accuracy.\n",
    "   - **Basic threshold-based** alerts for unusual footfall (e.g., X people in Y minutes).\n",
    "   - No face blurring or staff exclusion at first (though staff labeling might come later).\n",
    "   - No tamper detection for now.\n",
    "\n",
    "5. **User Interaction & Updates**\n",
    "   - System can **auto-update** from a repository for bug fixes or feature enhancements.\n",
    "   - **Calibration step** required to define region of interest (entrance lines) visually.\n",
    "   - No on-device GUI; Pi remains headless.\n",
    "\n",
    "6. **Future Scope**\n",
    "   - Possibility of **adding sales correlation** with a simple CSV-based approach later.\n",
    "   - Potential for **face recognition** or **cross-store analytics** in the future, but not a current priority.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd02bb35-8993-451a-95a8-d6ef6b28e0a7",
   "metadata": {},
   "source": [
    "# Summary of the Conversation\n",
    "\n",
    "1. **Project Concept**  \n",
    "   - The user wants an application that runs on a **Raspberry Pi 5** (4GB or possibly 8GB) to detect customers entering and exiting a store, using an IP camera as the video source.\n",
    "   - The goal is to provide **real-time detection** and generate analytics accessible through an **Android mobile app**.\n",
    "   - Short video clips of each entry/exit event should be stored **locally** on the Pi with a **FIFO** cleanup policy.\n",
    "   - Logs may be pushed online (e.g., Firebase) if feasible; otherwise, a local DB on the Pi suffices.\n",
    "   - The user wants **near 100% accuracy** with YOLO-based detection (no advanced re-identification at this stage).\n",
    "   - The UI design references **Cupertino-style** widgets, though exact color schemes and design details are flexible.\n",
    "\n",
    "2. **Technical & Functional Requirements**  \n",
    "   - **Multiple cameras** might be used (the user may upgrade to an 8GB Pi to handle parallel streams).\n",
    "   - A **calibration step** is needed to define entry/exit lines visually.\n",
    "   - **Offline caching** for the analytics on the Android app if connectivity fails.\n",
    "   - Basic threshold-based alerts for unusual footfall (e.g., X people in Y minutes).\n",
    "   - Future expansions possible, such as staff labeling or face recognition, but **not** in the initial build.\n",
    "   - The Pi should be **headless** and auto-update from a repository as needed.\n",
    "\n",
    "3. **Constraints and Preferences**  \n",
    "   - The user has a reliable **fiber connection** at the store, so bandwidth is not a major concern.\n",
    "   - The user wants an easy route to integrate **sales data** in the future (CSV-based).\n",
    "   - The user’s tolerance for errors is low—prefers minimal false positives/negatives.\n",
    "   - No major data privacy or face-blurring requirement at this stage.\n",
    "\n",
    "4. **Questions & Answers**  \n",
    "   - We clarified details about storing clips, logs, multiple cameras, advanced analytics, and user interface design.\n",
    "   - The user is open to local or online data storage, with a slight preference for Firebase if it’s free, else a local DB.\n",
    "   - The user confirmed interest in iterative model tuning if off-the-shelf YOLO is not sufficiently accurate.\n",
    "\n",
    "---\n",
    "\n",
    "# Proposed Schedule\n",
    "\n",
    "Below is a **rough, self-assigned timeline** (approximately **1–2 weeks** total), integrating the user’s needs:\n",
    "\n",
    "1. **Days 1–3: Environment Setup & Initial Architecture**\n",
    "   1. Create a dedicated project folder (on Windows dev machine).\n",
    "   2. Initialize Python environment (e.g., conda or virtualenv).\n",
    "   3. Install core packages (FastAPI, YOLO-based detection library, etc.).\n",
    "   4. Set up a simple scaffold:\n",
    "      - `main.py` with FastAPI (or any framework you prefer).\n",
    "      - A structure for camera streaming, detection, and database modules.\n",
    "   5. Test basic YOLO inference on sample data.\n",
    "\n",
    "2. **Days 4–5: Camera & Local Database Implementation**\n",
    "   1. Implement IP camera streaming in code.\n",
    "   2. Integrate detection logic with a minimal local DB or data structure.\n",
    "   3. Handle short video clip creation and **FIFO** policy (e.g., auto-delete older clips past a 1000-clip threshold).\n",
    "   4. Draft the boundary calibration logic (defining the region/lines in a visual or programmatic manner).\n",
    "\n",
    "3. **Days 6–7: Mobile App Prototype & Firebase Log Sync**\n",
    "   1. Implement a basic Android app to display counts and analytics.\n",
    "   2. If feasible, set up Firebase for log data push. Otherwise, integrate local DB only and plan for an offline caching model.\n",
    "   3. Build an API endpoint to supply analytics data to the mobile app (daily/hourly counts, footfall thresholds, etc.).\n",
    "   4. Integrate threshold-based alert triggers for “unusual footfall” in the application logic.\n",
    "\n",
    "4. **Days 8–9: Polishing & Testing**\n",
    "   1. Perform stress testing with multiple cameras if possible, or simulate concurrency.\n",
    "   2. Evaluate detection accuracy and tune thresholds or YOLO settings (e.g., confidence, NMS).\n",
    "   3. Verify the FIFO clip storage logic works as intended.\n",
    "   4. Implement basic offline caching in the Android app, ensuring it can show historical data if offline.\n",
    "\n",
    "5. **Days 10–12: Final Adjustments & Raspberry Pi Deployment**\n",
    "   1. Migrate to the Raspberry Pi 5 environment.\n",
    "   2. Optimize performance (downscale if CPU usage is high).\n",
    "   3. Set up auto-updates (pull from GitHub or private repo on startup).\n",
    "   4. Conduct final user acceptance testing (UAT) with real store scenarios.\n",
    "\n",
    "6. **Day 13–14 (Buffer)**\n",
    "   - Allocate final days for any leftover tasks, bug fixes, or minor feature additions.\n",
    "   - Gather user feedback, plan for next-phase expansions (like staff detection, face recognition, or advanced analytics).\n",
    "\n",
    "This schedule is flexible and can be adjusted if the **Raspberry Pi 5** delivery is delayed or if we encounter performance bottlenecks that need extra optimization time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8804745-5061-458c-a620-1a2dd9430dc7",
   "metadata": {},
   "source": [
    "**Front-End Developer:**  \n",
    "I see we’re devoting Days 6–7 for building the mobile app prototype and Firebase log sync, but that seems a bit short. I’ll need more time to implement offline caching, especially if the design calls for an elegant Cupertino-style UI with smooth transitions and real-time chart updates. Two days might be tight for a robust MVP on Android.\n",
    "\n",
    "**Back-End Developer:**  \n",
    "From my side, the schedule for setting up local clip storage and a FIFO cleanup policy is fine. However, I’m also worried about the advanced analytics logic. We only have a couple of days in that plan to finalize and test analytics, including the threshold-based alerts. I’d prefer more time to test concurrency if multiple cameras are attached.\n",
    "\n",
    "**Full-Stack Developer:**  \n",
    "I’m concerned about the final deployment steps, especially migrating to the Raspberry Pi environment and ensuring everything works in real time. Sometimes, hardware constraints surface last minute. We might need a full extra week or so for optimization and testing on the actual Pi. Days 10–12 might not be enough, especially if performance issues appear.\n",
    "\n",
    "**Designer:**  \n",
    "The schedule doesn’t mention much about design iteration or user experience testing. If we’re aiming for a “premium” look, we typically need iterative feedback cycles. There’s no explicit buffer for design revisions, user acceptance tests, or fine-tuning the UI. We’re basically mentioning it on Day 13–14 as a buffer, but design refinements often need more structured time.\n",
    "\n",
    "---\n",
    "\n",
    "**Critique:**  \n",
    "- The **front-end tasks** (Cupertino-style app with real-time updates, offline caching, etc.) are compressed into two days, which seems too little.  \n",
    "- **Advanced analytics** with threshold alerts and concurrency across cameras might require additional time.  \n",
    "- The **deployment** phase on Raspberry Pi can produce unforeseen problems that need a longer troubleshooting window.  \n",
    "- There is minimal mention of **iterative design** or feedback loops, which are crucial for a polished UI/UX.\n",
    "\n",
    "---\n",
    "\n",
    "## Proposed Revised Plan\n",
    "\n",
    "**Week 1: Setup, Architecture, and Initial Prototyping**  \n",
    "1. **Days 1–2**:  \n",
    "   - Finalize environment and architecture (folders, libraries).  \n",
    "   - Implement bare-bones detection logic (YOLO) on sample data.  \n",
    "   - Begin basic server/API structure.  \n",
    "\n",
    "2. **Days 3–4**:  \n",
    "   - Work on local storage approach (DB + FIFO clip logic).  \n",
    "   - Create simple API endpoints for logs and events.  \n",
    "   - Start drafting the Android front-end skeleton, focusing on core UI flows (no advanced design yet).  \n",
    "\n",
    "3. **Days 5–7**:  \n",
    "   - Integrate real camera stream in a testing environment (downscale if necessary).  \n",
    "   - Implement calibration UI or process for entry/exit lines.  \n",
    "   - Basic analytics logic (daily/hourly counts) in the back-end.  \n",
    "   - Keep design minimal but functional to confirm data flow is correct.\n",
    "\n",
    "**Week 2: Refinement, Testing, and Deployment**  \n",
    "1. **Days 8–9**:  \n",
    "   - Refine the Android app with more advanced design elements (Cupertino styling).  \n",
    "   - Add threshold-based unusual footfall alerts.  \n",
    "   - Implement offline caching or Firebase sync as needed.  \n",
    "   - Begin thorough local testing with concurrency if multiple cameras are planned.  \n",
    "\n",
    "2. **Days 10–11**:  \n",
    "   - Deploy to Raspberry Pi, debug performance issues, and optimize downscaling or frame rates.  \n",
    "   - Test real-time detection in a mock store scenario.  \n",
    "   - Perform design review and minor UX revisions, ensuring smooth interactions.  \n",
    "\n",
    "3. **Days 12–14**:  \n",
    "   - **Buffer for final polish**: fix any Pi-specific issues, refine UI details, correct analytics edge cases.  \n",
    "   - **Iterative design feedback**: gather user input, revise screens, tweak color schemes, etc.  \n",
    "   - Ensure auto-update mechanism works.  \n",
    "\n",
    "This revised plan grants more time to each task, particularly front-end development, design refinement, and Pi-specific optimization. It also spaces out the workflow so we can test each major component (camera streaming, detection, analytics, UI) well before final deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a20ffb-f620-400a-b24e-036e770663ad",
   "metadata": {},
   "source": [
    "# Detailed Checklist: Days 1–2\n",
    "\n",
    "1. **Environment Setup & Architecture**\n",
    "   - [x] **Create or choose a project folder** (e.g., `edge_detection_app`).\n",
    "   - [x] **Initialize a Python environment** (e.g., using conda or virtualenv)\n",
    "     - Make sure Python 3.9+ is installed.\n",
    "   - [x] **Install required libraries** (e.g., FastAPI, YOLO detection library, etc.)\n",
    "   - [x] **Verify package installation**:\n",
    "     - Confirm versions of Python and each critical library (e.g., run `python --version` and `pip list`).\n",
    "   - [x] **Decide on code structure**:\n",
    "     - Plan primary folders (`app`, `models`, `tests`, etc.).\n",
    "     - Create empty placeholder files, e.g. `main.py`, `detection.py`, `database.py`.\n",
    "   - [x] **Set up a version control system** (e.g., Git):\n",
    "     - Create a repository for the project to track changes.\n",
    "\n",
    "2. **Implement Bare-Bones Detection Logic**\n",
    "   - [x] **Import the chosen YOLO model**:\n",
    "     - E.g., from ultralytics import YOLO\n",
    "     - Confirm it runs a quick test inference on a sample image or video.\n",
    "   - [x] **Create a simple function** (in `detection.py`) that:\n",
    "     - Loads the YOLO model (`yolov8n.pt` or another variant).\n",
    "     - Accepts an image/frame and returns detection results (bounding boxes, class labels).\n",
    "   - [x] **Test detection logic** locally:\n",
    "     - Use a sample input (static image or short video).\n",
    "     - Print or log the detection outputs (no advanced tracking or counting yet).\n",
    "   - [x] **Validate performance** on the dev machine:\n",
    "     - Check that the model runs without errors.\n",
    "     - Confirm detection speed is acceptable at a basic level.\n",
    "\n",
    "3. **Begin Basic Server/API Structure**\n",
    "   - [x] **Set up a minimal FastAPI app**:\n",
    "     - In `main.py`, create a `FastAPI()` instance.\n",
    "     - Add a basic health-check endpoint (e.g., GET `/ping` returns `{\"status\": \"ok\"}`).\n",
    "   - [x] **Create routes/endpoints** placeholders:\n",
    "     - For example, a `/detect` route that will eventually accept an image/frame.\n",
    "     - A `/calibrate` route placeholder for future boundary/line definitions.\n",
    "   - [x] **Run the server** to confirm it’s functional:\n",
    "     - Launch with `uvicorn main:app --reload` (assuming you have uvicorn installed).\n",
    "     - Visit `http://127.0.0.1:8000/ping` to verify the health-check endpoint.\n",
    "   - [x] **Document your progress**:\n",
    "     - Update README with instructions on how to run the server and test detection.\n",
    "\n",
    "By the end of **Days 1–2**, you should have:\n",
    "- A working local dev environment.\n",
    "- A skeleton code structure with a basic YOLO detection function on a test frame.\n",
    "- A minimal FastAPI app launching and responding at a test endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25497988-71ea-4e5f-9a3e-ad6258429497",
   "metadata": {},
   "source": [
    "# Detailed Checklist for Days 3–4\n",
    "\n",
    "## 1. Local Storage Approach (DB + FIFO Clip Logic)\n",
    "\n",
    "1. **Decide on Local DB**  \n",
    "   - [x] Choose a local database solution (e.g., SQLite) or confirm usage of existing DB.  \n",
    "   - [x] Create or update a `database.py` with functions to:\n",
    "     - Initialize tables for storing logs (timestamp, event type, etc.).  \n",
    "     - Insert new records (entry/exit events, clip metadata).  \n",
    "     - Retrieve logs for analytics or debugging.\n",
    "\n",
    "2. **Set Up Clip Storage Folder**  \n",
    "   - [x] Decide on a folder to store short video clips (e.g., `clips/`).  \n",
    "   - [x] Ensure the application can read/write to that folder (permission checks).\n",
    "\n",
    "3. **FIFO Clip Cleanup Logic**  \n",
    "   - [x] Implement a function or routine that periodically checks the number of stored clips.  \n",
    "   - [x] If the total count exceeds a threshold (e.g., 1000), remove the oldest clips first.  \n",
    "   - [x] Document or log each deletion for auditing.\n",
    "\n",
    "4. **Test DB and Clip Flow**  \n",
    "   - [x] Write a quick test to insert a “dummy event” into the DB.  \n",
    "   - [x] Store a dummy clip (could be a placeholder .mp4) in the `clips/` folder.  \n",
    "   - [x] Verify the cleanup logic triggers and removes older clips once the threshold is exceeded.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Create Simple API Endpoints for Logs & Events\n",
    "\n",
    "1. **Logs Endpoint**  \n",
    "   - [x] In `main.py` or a dedicated `routes/logs.py`, create an endpoint (e.g., `GET /logs`) to return stored logs/events in JSON format.  \n",
    "   - [x] Implement query parameters if needed (e.g., date range, event types).  \n",
    "   - [x] Ensure it queries the DB function (e.g., `fetch_logs()`) and returns structured data.\n",
    "\n",
    "2. **Events Endpoint**  \n",
    "   - [x] Add a route (e.g., `POST /events`) that can receive new event data from your detection code:\n",
    "     - For instance, an event might include the type (“entry” or “exit”), timestamp, and a path/ID referencing a clip.  \n",
    "   - [x] Validate the incoming data with Pydantic models (optional).  \n",
    "   - [x] Insert the event record into the DB, returning a success/failure message.\n",
    "\n",
    "3. **Basic Error Handling**  \n",
    "   - [x] Ensure endpoints handle database errors gracefully (e.g., use `try/except` blocks).  \n",
    "   - [x] Return descriptive error messages (e.g., “500 Internal Server Error” with a debug message if DB fails).\n",
    "\n",
    "4. **Test API Endpoints**  \n",
    "   - [x] Use a tool like cURL, Postman, or a web browser to:\n",
    "     - `POST /events` with sample data.  \n",
    "     - `GET /logs` to see if the inserted event shows up.  \n",
    "   - [x] Confirm JSON structure is correct and that the DB updates properly.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Start Drafting the Android Front-End Skeleton (Core UI Flows)\n",
    "\n",
    "1. **Initialize Android Project**  \n",
    "   - [x] Open Android Studio (or your preferred IDE) and create a new project (e.g., “ZVisionApp”).  \n",
    "   - [x] Choose a minimum Android SDK version that makes sense (e.g., API 21+).  \n",
    "   - [x] Use basic or empty activity as a starting point.\n",
    "\n",
    "2. **Core UI Layouts & Activities**  \n",
    "   - [x] Create a main activity (e.g., `MainActivity`) that has at least two basic UI components:\n",
    "     - A “Logs” button/link that navigates to a logs screen.  \n",
    "     - A “Dashboard” area that will later display real-time footfall counts (placeholder for now).\n",
    "   - [x] Create a second activity or fragment for viewing logs (e.g., `LogsActivity`).  \n",
    "     - Add a placeholder `RecyclerView` or text area to eventually show logs pulled from your API.\n",
    "\n",
    "3. **Networking Setup**  \n",
    "   - [x] Decide on a library to make HTTP calls (e.g., `Retrofit`, `OkHttp`).  \n",
    "   - [x] Implement a simple retrofit interface (or equivalent) to call your FastAPI endpoints:\n",
    "     - `GET /logs` → returns JSON  \n",
    "     - `POST /events` (if relevant for testing from mobile)\n",
    "   - [x] Test connectivity by retrieving “ping” from `GET /ping` (like a health check) and display the result in a TextView or Logcat.\n",
    "\n",
    "4. **UI Flow Verification**  \n",
    "   - [x] Launch the app on an emulator or device.  \n",
    "   - [x] Tap the “Logs” button to move to a logs screen (empty for now).  \n",
    "   - [x] Confirm no crashes when calling the `ping` endpoint from the app.\n",
    "\n",
    "5. **Keep the Design Minimal**  \n",
    "   - [x] No advanced design required yet, just placeholders.  \n",
    "   - [x] Add basic styling or layout constraints so you can easily expand later with Cupertino-styled widgets.\n",
    "\n",
    "---\n",
    "\n",
    "## Recap & Next Steps\n",
    "\n",
    "- By the end of **Days 3–4**, you should have:\n",
    "  1. **Local DB** structure in place with FIFO logic for clips.  \n",
    "  2. **API Endpoints** that let you insert and retrieve event/log data.  \n",
    "  3. **Android Front-End Skeleton** with minimal UI flows (main screen, logs screen) and a working health-check to your FastAPI server.\n",
    "\n",
    "- After completing these tasks, you’ll be positioned to:\n",
    "  - Integrate the **detection** logic with the new `/events` endpoint (so detection events automatically store in the DB).  \n",
    "  - Build out the logs screen in the Android app to display real logs from the server.  \n",
    "  - Enhance design elements, analytics, and more advanced detection features in subsequent steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c002ea4e-d198-415c-92cd-26de168a5bba",
   "metadata": {},
   "source": [
    "# Detailed Checklist: Days 5–7\n",
    "\n",
    "1. **Integrate Real Camera Stream**\n",
    "   - [ ] **Obtain the camera’s RTSP/HTTP URL** or local path for testing.\n",
    "   - [ ] **Update the detection pipeline** (in `counter.py` or similar) to accept a live stream rather than a static file.\n",
    "   - [x] **Add downscale/downsampling logic** if performance is too slow on test footage:\n",
    "     - [x] Decide on target resolution (e.g., 720p or 480p).\n",
    "     - [x] Verify YOLO detection speed vs. accuracy tradeoff.\n",
    "   - [ ] **Test the stream** end-to-end, ensuring frames are processed continuously.\n",
    "   - [ ] **Log any performance metrics** (FPS, CPU usage) for future optimizations.\n",
    "\n",
    "2. **Implement Calibration UI / Process (Entry/Exit Lines)**\n",
    "   - [ ] **Create a method** in your detection or UI code to define a polygon or lines:\n",
    "     - [ ] Provide a way to capture or display a reference frame from the camera.\n",
    "     - [ ] Let the user click/tap points (or drag lines) to specify entry/exit lines in a web UI or your Flutter app.\n",
    "   - [ ] **Store the calibration data** (line coordinates, polygons, etc.) in a config file or DB table:\n",
    "     - [ ] Ensure each store/camera combination has unique calibration settings.\n",
    "   - [ ] **Use the calibration** in your detection logic to determine if a bounding box crosses an entry or exit line.\n",
    "\n",
    "3. **Basic Analytics Logic (Daily/Hourly Counts)**\n",
    "   - [ ] **Extend your database schema** or queries if needed to calculate counts efficiently:\n",
    "     - [ ] Possibly add a separate “aggregation” table or simply query `entry_exit_events` by date/time.\n",
    "   - [ ] **Implement a function** (e.g., `get_daily_counts(store_id, date)`) that returns the total “entry” or “exit” events per hour/day.\n",
    "   - [ ] **Add an endpoint** (e.g., `/analytics/daily`) to fetch these aggregated stats in JSON form.\n",
    "   - [ ] **Test** by inserting sample events and confirming the aggregated results match expectations.\n",
    "\n",
    "4. **Keep Design Minimal but Functional**\n",
    "   - [ ] **Avoid big styling changes**; just ensure the UI flows are usable:\n",
    "     - [ ] The calibration screen or steps should be straightforward to access.\n",
    "     - [ ] The analytics or daily/hourly counts can appear in a basic list or chart placeholder.\n",
    "   - [ ] **Validate the data flow** from camera → detection → DB logging → analytics endpoint → front-end chart/text.\n",
    "   - [ ] **Document** any limitations or known issues (e.g., small performance dips, approximate line calibration, etc.).\n",
    "\n",
    "By following the above tasks in Days 5–7, you’ll have a working camera stream integration, an initial calibration process for entry/exit lines, basic analytics logic for daily/hourly counts, and a simple but functional user experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eda2900-a435-4579-bd35-33ec21ea7c0b",
   "metadata": {},
   "source": [
    "# Step-by-Step Guide: Adding Two Folders (videos, checkpoints) to .gitignore and Pushing Changes\n",
    "\n",
    "1. **Open or Create the `.gitignore` File**  \n",
    "   - In your project’s root directory, look for a file named `.gitignore`.  \n",
    "   - If it doesn’t exist, create one:\n",
    "     touch .gitignore\n",
    "\n",
    "2. **Add the Folders to `.gitignore`**  \n",
    "   - Append these lines to your `.gitignore` file:\n",
    "     videos/\n",
    "     checkpoints/\n",
    "   - This tells Git to ignore any files within `videos/` and `checkpoints/` folders during commits.\n",
    "\n",
    "3. **Remove Already-Tracked Files from Git**  \n",
    "   - If `videos/` or `checkpoints/` contents are **already committed** to the repo, you must untrack them:\n",
    "     git rm -r --cached videos\n",
    "     git rm -r --cached checkpoints\n",
    "   - This removes those folders from Git’s index (so they won’t appear in future commits) but **leaves them on your disk**.\n",
    "\n",
    "4. **Commit the Updated `.gitignore` and Removal**  \n",
    "   - Stage all changes (including the .gitignore changes and the untracking of folders):\n",
    "     git add .gitignore\n",
    "     git commit -m \"Ignore videos and checkpoints folders\"\n",
    "\n",
    "5. **Push the Changes to GitHub**  \n",
    "   - Send your updated commit to the remote:\n",
    "     git push origin master\n",
    "   - This commit removes the `videos/` and `checkpoints/` content from your remote repository (if it was previously tracked) and ensures they remain untracked going forward.\n",
    "\n",
    "6. **Confirm `.gitignore` is Working**  \n",
    "   - From now on, Git will ignore any new or modified files inside `videos/` and `checkpoints/`.  \n",
    "   - If you do `git status`, you should **not** see changes from those folders anymore.\n",
    "\n",
    "---\n",
    "**Important Note**:  \n",
    "- Once `.gitignore` is set, any **new** files in `videos/` or `checkpoints/` won’t be tracked by Git.  \n",
    "- Any **existing** large files that were previously committed to the remote might still be in the repository history. If you need to remove them from GitHub’s history (e.g., to reduce repo size), you’ll have to rewrite commits with a tool like BFG Repo-Cleaner. Otherwise, ignoring them now only prevents future commits, but doesn’t erase old data from the remote.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0481e497-8172-46ac-b387-5bf9bc9da69e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
