{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c58f0a3e-7b19-4187-9aa2-374271e07a79",
   "metadata": {},
   "source": [
    "# Step-1: Detailed Documentation\n",
    "\n",
    "Below is a comprehensive, step-by-step guide on how we will create our premium-looking **Customer Entry-Exit Detection** application from scratch. We will target a **Raspberry Pi 5 (4GB)** for final deployment, using an **IP camera** to capture real-time video feeds, and generate analytics to help store owners analyze their **sales/walk-in ratio** and related metrics. The entire development cycle is estimated at **1–2 weeks**. \n",
    "\n",
    "---\n",
    "## 1. Project Concept and Goals\n",
    "\n",
    "1. **Real-time Detection**: We need a system that continuously processes the live feed from an IP camera and detects each customer entering or leaving the store.\n",
    "2. **Analytics & Metrics**: Store owners can view aggregated statistics such as total daily in/out counts, hourly trends, and possibly advanced ratios like sales-to-footfall.\n",
    "3. **Edge Deployment**: The final solution runs on a **Raspberry Pi 5 (4GB)**, ensuring the system can operate at the store location without requiring a large, remote server.\n",
    "\n",
    "---\n",
    "## 2. Preliminary Design Outline\n",
    "\n",
    "1. **Core Components**:\n",
    "   - **Camera Connection**: A streaming client or library that retrieves frames from the IP camera.\n",
    "   - **Detection Module**: An object detection model (e.g., based on YOLO or similar) to detect people in frames.\n",
    "   - **Tracking & Counting**: Logic to maintain each detected object’s identity, track movements, and label them as “entering” or “exiting” based on boundary crossing.\n",
    "   - **Database & Analytics**: A place to store each event and generate aggregated metrics (daily counts, peak hours, etc.).\n",
    "   - **Frontend & Display**: Simple or “premium” interface that can be accessed locally or via network, showing real-time counts and historical data with a Cupertino-style or other modern UI design.\n",
    "\n",
    "2. **Data Flow** (High-level):\n",
    "   1. **IP Camera → Frame Grab**  \n",
    "      The Raspberry Pi (or your dev machine) retrieves the video feed.\n",
    "   2. **Object Detection**  \n",
    "      Each video frame is passed into a detection model to locate and classify people.\n",
    "   3. **Boundary/Line Crossing**  \n",
    "      A geometric logic determines if the person crossed an “in” line, an “out” line, or stayed inside a region of interest.\n",
    "   4. **Count & Log**  \n",
    "      Each event is logged (with timestamp, direction in/out, and possibly short clips or snapshots if needed).\n",
    "   5. **Analytics**  \n",
    "      Aggregated data is processed (hourly totals, daily, monthly, etc.) and displayed to the store owner.\n",
    "\n",
    "---\n",
    "## 3. Development Schedule (Approx. 1–2 Weeks)\n",
    "\n",
    "### Days 1–3: Environment Setup & File Structure\n",
    "\n",
    "1. **Prepare Your Dev Environment (Windows System + Anaconda + VS Code)**:\n",
    "   - Create a dedicated project folder. For instance: `edge_detection_app`.\n",
    "   - Make a new environment (to avoid conflicts with other packages). For example:\n",
    "     - Use something like:  \n",
    "       `conda create --name edge_app python=3.9`\n",
    "     - Then activate it:  \n",
    "       `conda activate edge_app`\n",
    "   - Install any critical libraries we expect to use:  \n",
    "       - Example: `pip install fastapi uvicorn opencv-python numpy`  \n",
    "       - If using YOLO-based approaches, we might install `ultralytics` or another detection framework.\n",
    "   - Confirm that your Python, pip, and environment are set up correctly.\n",
    "\n",
    "2. **Plan Directory Layout**:  \n",
    "   - **app/**  \n",
    "     - `main.py` (entry point for our FastAPI or any server code)  \n",
    "     - `camera_client.py` (manages IP camera connection)  \n",
    "     - `detection.py` (YOLO or other detection logic)  \n",
    "     - `counter.py` (tracking and line-crossing logic)  \n",
    "     - `analytics.py` (summaries and stats generation)  \n",
    "     - `database.py` (DB connection and queries)  \n",
    "     - `routes/` (organized FastAPI routes, if we prefer separate files)  \n",
    "   - **models/**  \n",
    "     - Pydantic schemas or data classes.  \n",
    "   - **static/** or **frontend/** (if any UI assets are needed)  \n",
    "   - **tests/** (test suite for each module)\n",
    "\n",
    "3. **Raspberry Pi Preparation** (Planned, not done yet):\n",
    "   - Once the Pi arrives (in about 3–4 days), we’ll replicate the environment on it.\n",
    "   - We can either replicate the exact conda environment or use a simpler approach (e.g., `pip` in a virtualenv) to ensure minimal overhead on the Pi.\n",
    "\n",
    "### Days 4–7: Core Detection & Counting Logic (High-Level Future Tasks)\n",
    "- (Not implementing yet—just a placeholder for the plan.)\n",
    "- Integrate the camera feed, run object detection on each frame, track recognized bounding boxes, and decide in/out statuses.\n",
    "\n",
    "### Days 8–12: UI & Analytics (High-Level Future Tasks)\n",
    "- (Placeholder for planning.)\n",
    "- Build a modern interface or an API returning JSON endpoints that a “premium-style” front-end can consume. Provide analytics, charts, and peak hour metrics.\n",
    "\n",
    "---\n",
    "## 4. Step-by-Step Guide (Expanded)\n",
    "\n",
    "1. **Setting Up the Dev Environment**:\n",
    "   - On Windows, open **VS Code** and your chosen folder (`edge_detection_app`).\n",
    "   - Start a terminal in VS Code, activate your environment, and confirm everything with `python --version`.\n",
    "\n",
    "2. **Establish a Starter FastAPI Project**:\n",
    "   - Initialize a `main.py` with the bare bones of a FastAPI server:\n",
    "     - ```python\n",
    "       from fastapi import FastAPI\n",
    "\n",
    "       app = FastAPI()\n",
    "\n",
    "       @app.get(\"/\")\n",
    "       def read_root():\n",
    "           return {\"message\": \"Hello from Edge Detection App!\"}\n",
    "       ```\n",
    "   - Test run with `uvicorn main:app --reload`. Verify in your browser at `http://127.0.0.1:8000`.\n",
    "\n",
    "3. **Plan for the Database**:\n",
    "   - Depending on your preference, set up either:\n",
    "     - A lightweight **SQLite** database if your local storage needs are minimal.\n",
    "     - A remote or local **PostgreSQL** for advanced features, especially if big data or concurrency is expected.\n",
    "   - For now, plan to create a `database.py` with functions for connecting and performing basic CRUD operations.\n",
    "\n",
    "4. **Outline the Detection Approach**:\n",
    "   - Decide on a detection model. For example, if we use **ultralytics YOLO**, we can do:\n",
    "     - `pip install ultralytics`\n",
    "     - Then in `detection.py`:\n",
    "       ```python\n",
    "       from ultralytics import YOLO\n",
    "\n",
    "       def load_model():\n",
    "           model = YOLO('yolov8n.pt')  # or a custom model\n",
    "           return model\n",
    "       ```\n",
    "   - Outline how you’ll capture frames from the IP camera (e.g., `rtsp://` URL). Keep in mind that we’ll finalize this when we have the Pi + camera.\n",
    "\n",
    "5. **Prototype Tracking & Counting**:\n",
    "   - In `counter.py`, write a class or function that:\n",
    "     - Receives frames, identifies bounding boxes for people, and uses a “tracking ID” approach (like a built-in YOLO tracker or custom).\n",
    "     - Identifies if a bounding box crosses a specific line or polygon boundary for “entry” vs. “exit.”\n",
    "     - Logs events to the database.\n",
    "\n",
    "6. **Validation & Local Testing**:\n",
    "   - Use a test video feed (locally stored or a dummy RTSP stream) to ensure the detection pipeline works well on your Windows machine.\n",
    "\n",
    "7. **Raspberry Pi Deployment** (Days 4+ or once Pi is available):\n",
    "   - Transfer the code to the Pi.\n",
    "   - Install Python dependencies (consider whether conda is practical or if you prefer a simpler approach).\n",
    "   - Adjust the detection logic if any performance constraints arise (might switch to a smaller model or optimize settings).\n",
    "\n",
    "8. **Analytics Implementation**:\n",
    "   - Create a route or method that aggregates data by day/hour and returns the results in JSON (or renders via a UI template).\n",
    "   - Possibly integrate charts (via a web UI) so the store owner can see real-time in/out data, daily peaks, or advanced stats.\n",
    "\n",
    "---\n",
    "## 5. Project Focus for Days 1–3\n",
    "\n",
    "1. **Day 1**:\n",
    "   - Confirm environment setup, build skeleton folder structure.\n",
    "   - Initialize `main.py` with a basic FastAPI app.\n",
    "   - Discuss or decide on the detection framework (YOLO, etc.).\n",
    "\n",
    "2. **Day 2**:\n",
    "   - Implement a minimal `database.py` with a simple SQLite or other DB connection.\n",
    "   - Add placeholders for `detection.py` and `counter.py`.\n",
    "   - Make sure you can run `uvicorn main:app --reload` with no errors.\n",
    "\n",
    "3. **Day 3**:\n",
    "   - Test a simple local video detection flow.\n",
    "   - Finalize how we’ll handle IP camera streams in code (test a placeholder RTSP link).\n",
    "   - Prepare all placeholders so that once the Raspberry Pi arrives, we can quickly integrate and test on the actual device + camera.\n",
    "\n",
    "---\n",
    "## 6. Next Steps\n",
    "\n",
    "This document gives a thorough, high-level blueprint of how we’ll create the **new application** from scratch. Our next steps after finalizing this plan include:\n",
    "\n",
    "- Completing **Day 1–3** tasks (environment, skeleton code, local testing).\n",
    "- Extending the codebase with real detection, tracking, database logging, and eventually analytics once the Pi and camera are available.\n",
    "\n",
    "This concludes **Step-1**: our detailed documentation and plan to move forward. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f660c-a254-4db8-b014-ee8c6a1f4ff8",
   "metadata": {},
   "source": [
    "**question**: What database solution do you prefer for storing event logs—SQLite, PostgreSQL, or something else? \\\n",
    "**answer**: I don't have any preference. My plan is to have the raspberry pi for inference and video storage, while the logs are pushed online such as firebase (if it is feasible).\n",
    "\n",
    "**question**: How do you plan to access the analytics dashboard (web browser on local network, remote access, etc.)? \\\n",
    "**answer**: Analytics dashboard on the mobile app. \n",
    "\n",
    "**question**: Do you have specific security or authentication requirements for the dashboard (e.g., user roles, login system)? \\\n",
    "**answer**: No. The app will be deployed only to the admin who will have access to all features.\n",
    "\n",
    "**question**: Will the analytics include only basic counts and hourly/daily summaries, or do you need more advanced statistics (e.g., dwell times, repeat visits)? \\\n",
    "**answer**: I need more advanced summaries. But, note that, I don't want a complicated system with re-identification.\n",
    "\n",
    "**question**: What resolution and frame rate can your IP camera reliably provide over your network? \\\n",
    "**answer**: It provides 30FPS 1080P resolution. If the edge device cannot handle, please make sure, downscaling and downrating are done.\n",
    "\n",
    "**question**: Are there any constraints on CPU usage, GPU availability, or power consumption for the Raspberry Pi? \\\n",
    "**answer**: Get the specs of raspberry pi 5 4GB device as it will be dedicated only for this task.\n",
    "\n",
    "**question**: Do you want the system to store short video clips of each detected entry/exit event, or just text-based logs? \\\n",
    "**answer**: Yes, I need to store short clips\n",
    "\n",
    "**question**: Which object detection model are you comfortable with—YOLO variants, MobileNet, or do you have a specific preference? \\\n",
    "**answer**: YOLO is fine as I have tested it. Not sure about MobileNet or other models.\n",
    "\n",
    "**question**: Do you expect to run additional processes (like POS software) on the Raspberry Pi, which might compete for resources? \\\n",
    "**answer**: No. Edge device is dedicated to this single application.\n",
    "\n",
    "**question**: Should we provide a fallback or offline mode if the network connection to the IP camera is unstable? \\\n",
    "**answer**: Not sure about it.\n",
    "\n",
    "**question**: How long do you need to retain data (e.g., logs, analytics) before archiving or purging? \\\n",
    "**answer**: Not sure about it.\n",
    "\n",
    "**question**: Do you need any integration with other systems, such as a sales database, to correlate footfall with sales data? \\\n",
    "**answer**: Not right now, but it is on the cards. I would like the code to be modular, so that I can just plug it in when needed.\n",
    "\n",
    "**question**: Will you require any custom UI design elements beyond the Cupertino-themed widgets mentioned (e.g., brand colors, logos)? \\\n",
    "**answer**: Not right now.\n",
    "\n",
    "**question**: Are there any privacy or data-protection regulations (like GDPR) that we need to consider, especially for captured videos or images? \\\n",
    "**answer**: Not sure about that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c0f8bd-d5b7-443a-b712-27011579f1cc",
   "metadata": {},
   "source": [
    "**question**: Since you plan to use Firebase, are you comfortable with potential subscription costs or data limits, and do you already have a Firebase account set up? \\\n",
    "**answer**: Isn't the spark (free) plan sufficient for this? If not, I would easily have the DB in the raspberry pi using other DB.\n",
    "\n",
    "**question**: For storing short video clips locally on the Pi, how much storage capacity is available, and do we need an automated cleanup policy? \\\n",
    "**answer**: Yes. I need a threshold like 1000 clips, followed by first-in-first-out cleanup policy.\n",
    "\n",
    "**question**: Should the mobile app be a native iOS/Android app, or a cross-platform framework, and do you already have a preferred approach? \\\n",
    "**answer**: We'll start with android app.\n",
    "\n",
    "**question**: Do you need real-time notifications (e.g., push alerts on your phone) whenever a person enters or exits? \\\n",
    "**answer**: Not whenever person enters or exits. But, need alerts scenarios like but not limited to when unusual footfall detection. \n",
    "\n",
    "**question**: Would you like to anonymize video clips in any way (e.g., face blurring) or store them as-is? \\\n",
    "**answer**: Store them as it is.\n",
    "\n",
    "**question**: For advanced summaries, which metrics are highest priority (e.g., average visit duration, peak traffic hours, conversion rate if we have sales data later)? \\\n",
    "**answer**: You do a deep brainstorm and give a set of metrics that makes sense.\n",
    "\n",
    "**question**: Could you clarify if you’d like an on-device UI (e.g., touchscreen on the Pi) in addition to the mobile app, or will the Pi only run headless? \\\n",
    "**answer**: No. I need Pi to be headless.\n",
    "\n",
    "**question**: Should the system prompt you to manually update or automatically pull updates from a repository (for feature enhancements or bug fixes)? \\\n",
    "**answer**: Auto update.\n",
    "\n",
    "**question**: What network setup do you have at the store—wired Ethernet, reliable Wi-Fi, or do we need fallback to something like a 4G dongle? \\\n",
    "**answer**: Reliable fiber connection with wifi.\n",
    "\n",
    "**question**: For downscaling video, do you have a target resolution or frame rate in mind to ensure smooth object detection on the Pi? \\\n",
    "**answer**: No, let it be a hyperparameter.\n",
    "\n",
    "**question**: When storing event logs in Firebase, do you also want local caching of logs if internet is unavailable, and then push them when online again? \\\n",
    "**answer**: Yes. I need it. (Even if firebase is not used.)\n",
    "\n",
    "**question**: Do you plan to maintain multiple IP cameras in the future (e.g., multiple store entrances) under one Pi device? \\\n",
    "**answer**: Yes. I was about to ask you if it is possible. If multiple entrance can be processed on one pi device, I would buy 8GB instead of 4GB device."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2501455-158a-4b0e-b040-c33f7ae5c79e",
   "metadata": {},
   "source": [
    "**question**: Would you like to keep logs for each camera separately, or aggregate them in one unified dataset when you have multiple entrances? \\\n",
    "**answer**: Keep logs of each camera separately.\n",
    "\n",
    "**question**: If you opt for 8GB instead of 4GB on the Pi, do you have any additional performance targets—like higher resolution processing, or more AI models running in parallel? \\\n",
    "**answer**: Parallel running is necessary right? To run multiple entrance simultaneously. I don't have any other performance target like higher res processing.\n",
    "\n",
    "**question**: For unusual footfall detection alerts, do you want basic thresholds (e.g., X people in Y minutes) or something more sophisticated (e.g., sudden surges based on historical average)? \\\n",
    "**answer**: Basic is fine.\n",
    "\n",
    "**question**: Regarding the clips stored on the Pi, should we also consider automatically uploading them to the cloud (Firebase or elsewhere) after each event, or only keep them locally? \\\n",
    "**answer**: No, let it just be local. \n",
    "\n",
    "**question**: If the app runs on Android, do you want any offline capabilities there (caching old analytics or logs on the device), or is an online connection required to view analytics? \\\n",
    "**answer**: Offline cache should be there.\n",
    "\n",
    "**question**: Are there any UI preferences for the mobile app beyond Cupertino-style widgets, such as specific color schemes or minimalistic layouts? \\\n",
    "**answer**: We'll discuss during building the application.\n",
    "\n",
    "**question**: For the advanced summaries you mentioned, do you want daily/hourly segmentation, trends over weeks/months, or real-time charts that refresh as events occur? \\\n",
    "**answer**: I am not sure about it now.\n",
    "\n",
    "**question**: What level of error tolerance do you envision for the detection? For instance, do you allow for some false positives/negatives, or do you need near 100% accuracy? \\\n",
    "**answer**: I need near 100% accuracy with negligable FP/FN.\n",
    "\n",
    "**question**: If you decide to add sales data later, do you have a preferred POS or database format we should anticipate integrating with? \\\n",
    "**answer**: Not sure, but, let's stick for csv now as it is simple.\n",
    "\n",
    "**question**: Finally, is there any scenario where the Raspberry Pi will need a display in future (e.g., quick local debugging), or shall we always assume strict headless operation? \\\n",
    "**answer**: No, just headless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38a2be5-46b9-45ea-b043-a8086d268080",
   "metadata": {},
   "source": [
    "**question**: Does the store layout or camera positioning present any challenging angles or lighting conditions (e.g., glare, reflections, partially obstructed entrances) that we should account for? \\\n",
    "**answer**: No.\n",
    "\n",
    "**question**: Would you like to exclude staff from the counting or otherwise differentiate them (if feasible) to get more accurate customer-only data? \\\n",
    "**answer**: Not right now. But as the video capture grows, I will start labeling staff / non-staff and do fine-tuning to train on the store data for the classification.\n",
    "\n",
    "**question**: Do you anticipate high-density crowds (e.g., a rush hour), and should the system handle partial occlusions or group entries? \\\n",
    "**answer**: Max 3-4 people at any point of time walking through the store.\n",
    "\n",
    "**question**: Are there any constraints related to internet bandwidth for sending logs/clips, or is your connection robust enough for all planned operations? \\\n",
    "**answer**: Bandwidth is robust.\n",
    "\n",
    "**question**: Do you want a simple calibration step upon setup (e.g., specify the entrance lines in a visual interface) or should we hard-code the region of interest? \\\n",
    "**answer**: I need the calibration step.\n",
    "\n",
    "**question**: Since you aim for near 100% detection accuracy, are you open to iterative tuning (e.g., fine-tuning the model on store-specific data) if the default YOLO model underperforms? \\\n",
    "**answer**: Yes. (Refer answer of question-2 in this message.)\n",
    "\n",
    "**question**: How do you plan to manage user support or troubleshooting? Should we provide a remote debugging interface or logs for diagnostics? \\\n",
    "**answer**: Not sure.\n",
    "\n",
    "**question**: If multiple cameras stream to the same Pi, do you want each feed processed in parallel on separate threads/processes for real-time performance? \\\n",
    "**answer**: Not sure.\n",
    "\n",
    "**question**: Do you want any kind of tamper detection (e.g., alerts if the camera is covered or disconnected)? \\\n",
    "**answer**: Not right now.\n",
    "\n",
    "**question**: Finally, are there any expansion ideas or future plans (e.g., face recognition for VIP customers, cross-store analytics) that might influence our initial architecture? \\\n",
    "**answer**: Yes maybe, but don't focus on it right now.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fbcda6-41e1-4ce0-bce3-f7e6c9fd7e6c",
   "metadata": {},
   "source": [
    "## Summary of Requirements and Preferences\n",
    "\n",
    "1. **Data Storage & Backend**\n",
    "   - Primary logs can be **pushed online** (Firebase) if feasible; otherwise, use a local database on the Pi.\n",
    "   - **Short video clips** will be stored locally on the Pi, with a **FIFO cleanup** policy after reaching 1000 clips.\n",
    "\n",
    "2. **Mobile App & Analytics**\n",
    "   - Analytics on an **Android** mobile app (Cupertino-style UI discussion later).\n",
    "   - Requires **offline caching** of analytics/logs when no internet.\n",
    "   - No complicated authentication—admin has all access.\n",
    "   - **Advanced summaries** beyond basic counts, but **no re-identification** for staff vs. customers initially.\n",
    "   - Potential for **iterative tuning** with store-specific data to improve accuracy.\n",
    "\n",
    "3. **Hardware & Performance**\n",
    "   - Target device is a **Raspberry Pi 5** (4GB or 8GB). Pi runs **headless** and is dedicated solely to this task.\n",
    "   - **IP camera** provides **1080p at 30FPS**; plan to **downscale**/downrate if needed for performance.\n",
    "   - **Multiple camera feeds** may be handled in parallel if the Pi resources allow.\n",
    "\n",
    "4. **Detection & Features**\n",
    "   - Uses **YOLO** for object detection. Aiming for **near 100%** accuracy.\n",
    "   - **Basic threshold-based** alerts for unusual footfall (e.g., X people in Y minutes).\n",
    "   - No face blurring or staff exclusion at first (though staff labeling might come later).\n",
    "   - No tamper detection for now.\n",
    "\n",
    "5. **User Interaction & Updates**\n",
    "   - System can **auto-update** from a repository for bug fixes or feature enhancements.\n",
    "   - **Calibration step** required to define region of interest (entrance lines) visually.\n",
    "   - No on-device GUI; Pi remains headless.\n",
    "\n",
    "6. **Future Scope**\n",
    "   - Possibility of **adding sales correlation** with a simple CSV-based approach later.\n",
    "   - Potential for **face recognition** or **cross-store analytics** in the future, but not a current priority.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd02bb35-8993-451a-95a8-d6ef6b28e0a7",
   "metadata": {},
   "source": [
    "# Summary of the Conversation\n",
    "\n",
    "1. **Project Concept**  \n",
    "   - The user wants an application that runs on a **Raspberry Pi 5** (4GB or possibly 8GB) to detect customers entering and exiting a store, using an IP camera as the video source.\n",
    "   - The goal is to provide **real-time detection** and generate analytics accessible through an **Android mobile app**.\n",
    "   - Short video clips of each entry/exit event should be stored **locally** on the Pi with a **FIFO** cleanup policy.\n",
    "   - Logs may be pushed online (e.g., Firebase) if feasible; otherwise, a local DB on the Pi suffices.\n",
    "   - The user wants **near 100% accuracy** with YOLO-based detection (no advanced re-identification at this stage).\n",
    "   - The UI design references **Cupertino-style** widgets, though exact color schemes and design details are flexible.\n",
    "\n",
    "2. **Technical & Functional Requirements**  \n",
    "   - **Multiple cameras** might be used (the user may upgrade to an 8GB Pi to handle parallel streams).\n",
    "   - A **calibration step** is needed to define entry/exit lines visually.\n",
    "   - **Offline caching** for the analytics on the Android app if connectivity fails.\n",
    "   - Basic threshold-based alerts for unusual footfall (e.g., X people in Y minutes).\n",
    "   - Future expansions possible, such as staff labeling or face recognition, but **not** in the initial build.\n",
    "   - The Pi should be **headless** and auto-update from a repository as needed.\n",
    "\n",
    "3. **Constraints and Preferences**  \n",
    "   - The user has a reliable **fiber connection** at the store, so bandwidth is not a major concern.\n",
    "   - The user wants an easy route to integrate **sales data** in the future (CSV-based).\n",
    "   - The user’s tolerance for errors is low—prefers minimal false positives/negatives.\n",
    "   - No major data privacy or face-blurring requirement at this stage.\n",
    "\n",
    "4. **Questions & Answers**  \n",
    "   - We clarified details about storing clips, logs, multiple cameras, advanced analytics, and user interface design.\n",
    "   - The user is open to local or online data storage, with a slight preference for Firebase if it’s free, else a local DB.\n",
    "   - The user confirmed interest in iterative model tuning if off-the-shelf YOLO is not sufficiently accurate.\n",
    "\n",
    "---\n",
    "\n",
    "# Proposed Schedule\n",
    "\n",
    "Below is a **rough, self-assigned timeline** (approximately **1–2 weeks** total), integrating the user’s needs:\n",
    "\n",
    "1. **Days 1–3: Environment Setup & Initial Architecture**\n",
    "   1. Create a dedicated project folder (on Windows dev machine).\n",
    "   2. Initialize Python environment (e.g., conda or virtualenv).\n",
    "   3. Install core packages (FastAPI, YOLO-based detection library, etc.).\n",
    "   4. Set up a simple scaffold:\n",
    "      - `main.py` with FastAPI (or any framework you prefer).\n",
    "      - A structure for camera streaming, detection, and database modules.\n",
    "   5. Test basic YOLO inference on sample data.\n",
    "\n",
    "2. **Days 4–5: Camera & Local Database Implementation**\n",
    "   1. Implement IP camera streaming in code.\n",
    "   2. Integrate detection logic with a minimal local DB or data structure.\n",
    "   3. Handle short video clip creation and **FIFO** policy (e.g., auto-delete older clips past a 1000-clip threshold).\n",
    "   4. Draft the boundary calibration logic (defining the region/lines in a visual or programmatic manner).\n",
    "\n",
    "3. **Days 6–7: Mobile App Prototype & Firebase Log Sync**\n",
    "   1. Implement a basic Android app to display counts and analytics.\n",
    "   2. If feasible, set up Firebase for log data push. Otherwise, integrate local DB only and plan for an offline caching model.\n",
    "   3. Build an API endpoint to supply analytics data to the mobile app (daily/hourly counts, footfall thresholds, etc.).\n",
    "   4. Integrate threshold-based alert triggers for “unusual footfall” in the application logic.\n",
    "\n",
    "4. **Days 8–9: Polishing & Testing**\n",
    "   1. Perform stress testing with multiple cameras if possible, or simulate concurrency.\n",
    "   2. Evaluate detection accuracy and tune thresholds or YOLO settings (e.g., confidence, NMS).\n",
    "   3. Verify the FIFO clip storage logic works as intended.\n",
    "   4. Implement basic offline caching in the Android app, ensuring it can show historical data if offline.\n",
    "\n",
    "5. **Days 10–12: Final Adjustments & Raspberry Pi Deployment**\n",
    "   1. Migrate to the Raspberry Pi 5 environment.\n",
    "   2. Optimize performance (downscale if CPU usage is high).\n",
    "   3. Set up auto-updates (pull from GitHub or private repo on startup).\n",
    "   4. Conduct final user acceptance testing (UAT) with real store scenarios.\n",
    "\n",
    "6. **Day 13–14 (Buffer)**\n",
    "   - Allocate final days for any leftover tasks, bug fixes, or minor feature additions.\n",
    "   - Gather user feedback, plan for next-phase expansions (like staff detection, face recognition, or advanced analytics).\n",
    "\n",
    "This schedule is flexible and can be adjusted if the **Raspberry Pi 5** delivery is delayed or if we encounter performance bottlenecks that need extra optimization time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8804745-5061-458c-a620-1a2dd9430dc7",
   "metadata": {},
   "source": [
    "**Front-End Developer:**  \n",
    "I see we’re devoting Days 6–7 for building the mobile app prototype and Firebase log sync, but that seems a bit short. I’ll need more time to implement offline caching, especially if the design calls for an elegant Cupertino-style UI with smooth transitions and real-time chart updates. Two days might be tight for a robust MVP on Android.\n",
    "\n",
    "**Back-End Developer:**  \n",
    "From my side, the schedule for setting up local clip storage and a FIFO cleanup policy is fine. However, I’m also worried about the advanced analytics logic. We only have a couple of days in that plan to finalize and test analytics, including the threshold-based alerts. I’d prefer more time to test concurrency if multiple cameras are attached.\n",
    "\n",
    "**Full-Stack Developer:**  \n",
    "I’m concerned about the final deployment steps, especially migrating to the Raspberry Pi environment and ensuring everything works in real time. Sometimes, hardware constraints surface last minute. We might need a full extra week or so for optimization and testing on the actual Pi. Days 10–12 might not be enough, especially if performance issues appear.\n",
    "\n",
    "**Designer:**  \n",
    "The schedule doesn’t mention much about design iteration or user experience testing. If we’re aiming for a “premium” look, we typically need iterative feedback cycles. There’s no explicit buffer for design revisions, user acceptance tests, or fine-tuning the UI. We’re basically mentioning it on Day 13–14 as a buffer, but design refinements often need more structured time.\n",
    "\n",
    "---\n",
    "\n",
    "**Critique:**  \n",
    "- The **front-end tasks** (Cupertino-style app with real-time updates, offline caching, etc.) are compressed into two days, which seems too little.  \n",
    "- **Advanced analytics** with threshold alerts and concurrency across cameras might require additional time.  \n",
    "- The **deployment** phase on Raspberry Pi can produce unforeseen problems that need a longer troubleshooting window.  \n",
    "- There is minimal mention of **iterative design** or feedback loops, which are crucial for a polished UI/UX.\n",
    "\n",
    "---\n",
    "\n",
    "## Proposed Revised Plan\n",
    "\n",
    "**Week 1: Setup, Architecture, and Initial Prototyping**  \n",
    "1. **Days 1–2**:  \n",
    "   - Finalize environment and architecture (folders, libraries).  \n",
    "   - Implement bare-bones detection logic (YOLO) on sample data.  \n",
    "   - Begin basic server/API structure.  \n",
    "\n",
    "2. **Days 3–4**:  \n",
    "   - Work on local storage approach (DB + FIFO clip logic).  \n",
    "   - Create simple API endpoints for logs and events.  \n",
    "   - Start drafting the Android front-end skeleton, focusing on core UI flows (no advanced design yet).  \n",
    "\n",
    "3. **Days 5–7**:  \n",
    "   - Integrate real camera stream in a testing environment (downscale if necessary).  \n",
    "   - Implement calibration UI or process for entry/exit lines.  \n",
    "   - Basic analytics logic (daily/hourly counts) in the back-end.  \n",
    "   - Keep design minimal but functional to confirm data flow is correct.\n",
    "\n",
    "**Week 2: Refinement, Testing, and Deployment**  \n",
    "1. **Days 8–9**:  \n",
    "   - Refine the Android app with more advanced design elements (Cupertino styling).  \n",
    "   - Add threshold-based unusual footfall alerts.  \n",
    "   - Implement offline caching or Firebase sync as needed.  \n",
    "   - Begin thorough local testing with concurrency if multiple cameras are planned.  \n",
    "\n",
    "2. **Days 10–11**:  \n",
    "   - Deploy to Raspberry Pi, debug performance issues, and optimize downscaling or frame rates.  \n",
    "   - Test real-time detection in a mock store scenario.  \n",
    "   - Perform design review and minor UX revisions, ensuring smooth interactions.  \n",
    "\n",
    "3. **Days 12–14**:  \n",
    "   - **Buffer for final polish**: fix any Pi-specific issues, refine UI details, correct analytics edge cases.  \n",
    "   - **Iterative design feedback**: gather user input, revise screens, tweak color schemes, etc.  \n",
    "   - Ensure auto-update mechanism works.  \n",
    "\n",
    "This revised plan grants more time to each task, particularly front-end development, design refinement, and Pi-specific optimization. It also spaces out the workflow so we can test each major component (camera streaming, detection, analytics, UI) well before final deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a20ffb-f620-400a-b24e-036e770663ad",
   "metadata": {},
   "source": [
    "# Detailed Checklist: Days 1–2\n",
    "\n",
    "1. **Environment Setup & Architecture**\n",
    "   - [x] **Create or choose a project folder** (e.g., `edge_detection_app`).\n",
    "   - [x] **Initialize a Python environment** (e.g., using conda or virtualenv)\n",
    "     - Make sure Python 3.9+ is installed.\n",
    "   - [x] **Install required libraries** (e.g., FastAPI, YOLO detection library, etc.)\n",
    "   - [x] **Verify package installation**:\n",
    "     - Confirm versions of Python and each critical library (e.g., run `python --version` and `pip list`).\n",
    "   - [x] **Decide on code structure**:\n",
    "     - Plan primary folders (`app`, `models`, `tests`, etc.).\n",
    "     - Create empty placeholder files, e.g. `main.py`, `detection.py`, `database.py`.\n",
    "   - [x] **Set up a version control system** (e.g., Git):\n",
    "     - Create a repository for the project to track changes.\n",
    "\n",
    "2. **Implement Bare-Bones Detection Logic**\n",
    "   - [x] **Import the chosen YOLO model**:\n",
    "     - E.g., from ultralytics import YOLO\n",
    "     - Confirm it runs a quick test inference on a sample image or video.\n",
    "   - [x] **Create a simple function** (in `detection.py`) that:\n",
    "     - Loads the YOLO model (`yolov8n.pt` or another variant).\n",
    "     - Accepts an image/frame and returns detection results (bounding boxes, class labels).\n",
    "   - [x] **Test detection logic** locally:\n",
    "     - Use a sample input (static image or short video).\n",
    "     - Print or log the detection outputs (no advanced tracking or counting yet).\n",
    "   - [x] **Validate performance** on the dev machine:\n",
    "     - Check that the model runs without errors.\n",
    "     - Confirm detection speed is acceptable at a basic level.\n",
    "\n",
    "3. **Begin Basic Server/API Structure**\n",
    "   - [x] **Set up a minimal FastAPI app**:\n",
    "     - In `main.py`, create a `FastAPI()` instance.\n",
    "     - Add a basic health-check endpoint (e.g., GET `/ping` returns `{\"status\": \"ok\"}`).\n",
    "   - [x] **Create routes/endpoints** placeholders:\n",
    "     - For example, a `/detect` route that will eventually accept an image/frame.\n",
    "     - A `/calibrate` route placeholder for future boundary/line definitions.\n",
    "   - [x] **Run the server** to confirm it’s functional:\n",
    "     - Launch with `uvicorn main:app --reload` (assuming you have uvicorn installed).\n",
    "     - Visit `http://127.0.0.1:8000/ping` to verify the health-check endpoint.\n",
    "   - [x] **Document your progress**:\n",
    "     - Update README with instructions on how to run the server and test detection.\n",
    "\n",
    "By the end of **Days 1–2**, you should have:\n",
    "- A working local dev environment.\n",
    "- A skeleton code structure with a basic YOLO detection function on a test frame.\n",
    "- A minimal FastAPI app launching and responding at a test endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71af8c8-65ca-48c5-bad4-ff11aed2d7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
